{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "from pathlib import Path\n",
    "\n",
    "model_ckpt = \"omermazig/videomae-base-finetuned-kinetics-finetuned-nba-binary-data-2-batch-50-epochs-new-database\"\n",
    "\n",
    "dataset_root_path = Path(r\"C:\\Users\\User\\PycharmProjects\\DL-Workshop\\new_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pipe = pipeline(\"video-classification\", model=model_ckpt)\n",
    "trained_model = pipe.model\n",
    "image_processor = pipe.image_processor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from transformers import AutoImageProcessor, VideoMAEForVideoClassification\n",
    "# \n",
    "# \n",
    "# image_processor = AutoImageProcessor.from_pretrained(model_ckpt)\n",
    "# trained_model = VideoMAEForVideoClassification.from_pretrained(model_ckpt)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import pytorchvideo.data\n",
    "\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    Normalize,\n",
    "    RandomShortSideScale,\n",
    "    RemoveKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample,\n",
    ")\n",
    "\n",
    "from torchvision.transforms import (\n",
    "    Compose,\n",
    "    Lambda,\n",
    "    RandomCrop,\n",
    "    RandomHorizontalFlip,\n",
    "    Resize,\n",
    ")\n",
    "\n",
    "mean = image_processor.image_mean\n",
    "std = image_processor.image_std\n",
    "if \"shortest_edge\" in image_processor.size:\n",
    "    height = width = image_processor.size[\"shortest_edge\"]\n",
    "else:\n",
    "    height = image_processor.size[\"height\"]\n",
    "    width = image_processor.size[\"width\"]\n",
    "resize_to = (height, width)\n",
    "\n",
    "num_frames_to_sample = trained_model.config.num_frames # Should be 16 for VideoMAE based\n",
    "\n",
    "# Validation and evaluation datasets' transformations.\n",
    "inference_transform = Compose(\n",
    "    [\n",
    "        ApplyTransformToKey(\n",
    "            key=\"video\",\n",
    "            transform=Compose(\n",
    "                [\n",
    "                    UniformTemporalSubsample(num_frames_to_sample),\n",
    "                    Lambda(lambda x: x / 255.0),\n",
    "                    Normalize(mean, std),\n",
    "                    Resize(resize_to),\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def run_inference(model, video, label):\n",
    "    \"\"\"Utility to run inference given a model and test video.\n",
    "    \n",
    "    The video is assumed to be preprocessed already.\n",
    "    \"\"\"\n",
    "    # (num_frames, num_channels, height, width)\n",
    "    perumuted_sample_test_video = video.permute(1, 0, 2, 3)\n",
    "\n",
    "    inputs = {\n",
    "        \"pixel_values\": perumuted_sample_test_video.unsqueeze(0),\n",
    "        \"labels\": torch.tensor(\n",
    "            [label]\n",
    "        ),  # this can be skipped if you don't have labels available.\n",
    "    }\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    model = model.to(device)\n",
    "\n",
    "    # forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    return logits"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "MAX_CLIP_DURATION = 6\n",
    "\n",
    "for dataset_type in ['val', 'test']:\n",
    "    # build datasets.\n",
    "    inference_dataset = pytorchvideo.data.Ucf101(\n",
    "        data_path=os.path.join(dataset_root_path, dataset_type),\n",
    "        # Abuse, but make_clip_sampler with max duration take the whole clip\n",
    "        clip_sampler=pytorchvideo.data.make_clip_sampler(\"uniform\", MAX_CLIP_DURATION), \n",
    "        decode_audio=False,\n",
    "        transform=inference_transform,\n",
    "    )\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for sample_val_video in tqdm(inference_dataset):\n",
    "        logits = run_inference(trained_model, sample_val_video[\"video\"], sample_val_video[\"label\"])\n",
    "        video_name = sample_val_video['video_name']\n",
    "        predicted_class_idx = logits.argmax(-1).item()\n",
    "        real_class_idx = sample_val_video[\"label\"]\n",
    "        predicted_class = trained_model.config.id2label[predicted_class_idx]\n",
    "        real_class = trained_model.config.id2label[real_class_idx]\n",
    "        # print(\"Real class:\", real_class)\n",
    "        # print(\"Predicted class:\", predicted_class)\n",
    "        if real_class_idx == predicted_class_idx:\n",
    "            correct += 1\n",
    "        else:\n",
    "            print(f\"\\033[91m Failed on {video_name}. Predicted {predicted_class}, but real label is {real_class}\")\n",
    "        total += 1\n",
    "\n",
    "    print(f\"\\033[92m Accuracy on {dataset_type.upper()} is {correct / total}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
