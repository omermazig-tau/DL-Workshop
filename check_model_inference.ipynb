{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "from pathlib import Path\n",
    "\n",
    "model_ckpt = \"omermazig/videomae-base-finetuned-kinetics-finetuned-nba-data-2-batch-5-epochs-399-train-vids-multilabel\"\n",
    "\n",
    "dataset_root_path = Path(r\"C:\\Users\\User\\Google Drive\\dataset_2_classes_small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pipe = pipeline(\"video-classification\", model=model_ckpt)\n",
    "trained_model = pipe.model\n",
    "image_processor = pipe.image_processor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from transformers import AutoImageProcessor, VideoMAEForVideoClassification\n",
    "# \n",
    "# \n",
    "# image_processor = AutoImageProcessor.from_pretrained(model_ckpt)\n",
    "# trained_model = VideoMAEForVideoClassification.from_pretrained(model_ckpt)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "video_extension = \"avi\"\n",
    "\n",
    "all_video_file_paths = (\n",
    "    list(dataset_root_path.glob(f\"train/*/*.{video_extension}\"))\n",
    "    + list(dataset_root_path.glob(f\"val/*/*.{video_extension}\"))\n",
    "    + list(dataset_root_path.glob(f\"test/*/*.{video_extension}\"))\n",
    ")\n",
    "all_video_file_paths[:5]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class_labels = sorted({path.parent.stem for path in all_video_file_paths})\n",
    "label2id = {label: i for i, label in enumerate(class_labels)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "class_mini_labels = sorted({mini_label for label in class_labels for mini_label in label.split('_')})\n",
    "multi_label2id = {label: i for i, label in enumerate(class_mini_labels)}\n",
    "id2multi_label = {i: label for label, i in multi_label2id.items()}\n",
    "\n",
    "print(f\"Unique classes: {list(label2id.keys())}.\")\n",
    "print(f\"Unique mini classes: {list(multi_label2id.keys())}.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pytorchvideo.data\n",
    "\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    Normalize,\n",
    "    RandomShortSideScale,\n",
    "    RemoveKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample,\n",
    ")\n",
    "\n",
    "from torchvision.transforms import (\n",
    "    Compose,\n",
    "    Lambda,\n",
    "    RandomCrop,\n",
    "    RandomHorizontalFlip,\n",
    "    Resize,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pytorchvideo.data.labeled_video_paths import LabeledVideoPaths\n",
    "import torch\n",
    "from typing import Type, Optional, Callable, Dict, Any\n",
    "from pytorchvideo.data import ClipSampler, LabeledVideoDataset\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MultiLabelLabeledDataset(LabeledVideoDataset):\n",
    "    def __init__(self, data_path: str, clip_sampler: ClipSampler,\n",
    "                 video_sampler: Type[torch.utils.data.Sampler] = torch.utils.data.RandomSampler,\n",
    "                 transform: Optional[Callable[[Dict[str, Any]], Dict[str, Any]]] = None, video_path_prefix: str = \"\",\n",
    "                 decode_audio: bool = True, decoder: str = \"pyav\"):\n",
    "        labeled_video_paths = LabeledVideoPaths.from_path(data_path)\n",
    "        labeled_video_paths.path_prefix = video_path_prefix\n",
    "        super().__init__(labeled_video_paths, clip_sampler, video_sampler, transform, decode_audio, decoder)\n",
    "        self.label_components = class_mini_labels\n",
    "\n",
    "    def convert_label_to_binary_vector(self, label):\n",
    "        components = id2label[label].split(\"_\")\n",
    "        binary_vector = [1 if component in components else 0 for component in self.label_components]\n",
    "        return binary_vector\n",
    "\n",
    "    def __next__(self):\n",
    "        example = super().__next__()\n",
    "        original_label = example[\"label\"]\n",
    "        binary_vector = self.convert_label_to_binary_vector(original_label)\n",
    "        example[\"label\"] = binary_vector\n",
    "        return example"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "mean = image_processor.image_mean\n",
    "std = image_processor.image_std\n",
    "if \"shortest_edge\" in image_processor.size:\n",
    "    height = width = image_processor.size[\"shortest_edge\"]\n",
    "else:\n",
    "    height = image_processor.size[\"height\"]\n",
    "    width = image_processor.size[\"width\"]\n",
    "resize_to = (height, width)\n",
    "\n",
    "num_frames_to_sample = trained_model.config.num_frames\n",
    "\n",
    "# Validation and Test datasets' transformations.\n",
    "inference_transform = Compose(\n",
    "    [\n",
    "        ApplyTransformToKey(\n",
    "            key=\"video\",\n",
    "            transform=Compose(\n",
    "                [\n",
    "                    UniformTemporalSubsample(num_frames_to_sample),\n",
    "                    Lambda(lambda x: x / 255.0),\n",
    "                    Normalize(mean, std),\n",
    "                    Resize(resize_to),\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "MAX_VIDEO_DURATION = 6\n",
    "\n",
    "def build_evaluate_dataset(dataset_type: str):\n",
    "    # Validation and evaluation datasets.\n",
    "    dataset = MultiLabelLabeledDataset(\n",
    "        data_path=os.path.join(dataset_root_path, dataset_type),\n",
    "        clip_sampler=pytorchvideo.data.make_clip_sampler(\"uniform\", MAX_VIDEO_DURATION),\n",
    "        decode_audio=False,\n",
    "        transform=inference_transform,\n",
    "    )\n",
    "    return dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"kuku\",\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    remove_unused_columns=False,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score, classification_report\n",
    "from transformers import EvalPrediction\n",
    "import torch\n",
    "\n",
    "\n",
    "# source: https://jesusleal.io/2021/04/21/Longformer-multilabel-classification/\n",
    "def multi_label_metrics(predictions, labels, threshold=0.5):\n",
    "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(predictions))\n",
    "    # next, use threshold to turn them into integer predictions\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= threshold)] = 1\n",
    "    # finally, compute metrics\n",
    "    y_true = labels\n",
    "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average='micro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    # return as dictionary\n",
    "    metrics = {'f1': f1_micro_average,\n",
    "               'roc_auc': roc_auc,\n",
    "               'accuracy': accuracy}\n",
    "    report = classification_report(y_true, y_pred, target_names=class_mini_labels)\n",
    "    print(report)\n",
    "    return metrics\n",
    "\n",
    "# the compute_metrics function takes a Named Tuple as input:\n",
    "# predictions, which are the logits of the model as Numpy arrays,\n",
    "# and label_ids, which are the ground-truth labels as Numpy arrays.\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions,\n",
    "                                           tuple) else p.predictions\n",
    "    result = multi_label_metrics(\n",
    "        predictions=preds,\n",
    "        labels=p.label_ids)\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def collate_fn(examples):\n",
    "    \"\"\"The collation function to be used by `Trainer` to prepare data batches.\"\"\"\n",
    "    # permute to (num_frames, num_channels, height, width)\n",
    "    pixel_values = torch.stack(\n",
    "        [example[\"video\"].permute(1, 0, 2, 3) for example in examples]\n",
    "    )\n",
    "    labels = torch.tensor([example[\"label\"] for example in examples], dtype=torch.float)\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def build_trainer(dataset):\n",
    "    trainer = Trainer(\n",
    "        trained_model,\n",
    "        args,\n",
    "        eval_dataset=dataset,\n",
    "        tokenizer=image_processor,\n",
    "        compute_metrics=compute_metrics,\n",
    "        data_collator=collate_fn,\n",
    "    )\n",
    "    return trainer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for inference_dataset_type in ['val', 'test']:\n",
    "    # build dataset.\n",
    "    inference_dataset = build_evaluate_dataset(inference_dataset_type)\n",
    "    # build trainer.\n",
    "    inference_trainer = build_trainer(inference_dataset)\n",
    "    # print results\n",
    "    print(f\"---------{inference_dataset_type}---------\")\n",
    "    results = inference_trainer.evaluate(inference_dataset)\n",
    "    display(results)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
