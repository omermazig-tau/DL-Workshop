{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "from pathlib import Path\n",
    "\n",
    "model_ckpt = \"omermazig/5-class-8-batch-50-epochs-2000-vid-multiclass\"\n",
    "\n",
    "dataset_root_path = Path(r\"C:\\Users\\User\\Google Drive\\dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pipe = pipeline(\"video-classification\", model=model_ckpt)\n",
    "trained_model = pipe.model\n",
    "image_processor = pipe.image_processor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from transformers import AutoImageProcessor, VideoMAEForVideoClassification\n",
    "# \n",
    "# \n",
    "# image_processor = AutoImageProcessor.from_pretrained(model_ckpt)\n",
    "# trained_model = VideoMAEForVideoClassification.from_pretrained(model_ckpt)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "video_extension = \"avi\"\n",
    "\n",
    "all_video_file_paths = (\n",
    "    list(dataset_root_path.glob(f\"train/*/*.{video_extension}\"))\n",
    "    + list(dataset_root_path.glob(f\"val/*/*.{video_extension}\"))\n",
    "    + list(dataset_root_path.glob(f\"test/*/*.{video_extension}\"))\n",
    ")\n",
    "all_video_file_paths[:5]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class_labels = sorted({path.parent.stem for path in all_video_file_paths})\n",
    "label2id = {label: i for i, label in enumerate(class_labels)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "print(f\"Unique classes: {list(label2id.keys())}.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Callable, Dict, List\n",
    "import pytorchvideo.data\n",
    "\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    Normalize,\n",
    "    RandomShortSideScale,\n",
    "    RemoveKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample,\n",
    ")\n",
    "\n",
    "from torchvision.transforms import (\n",
    "    Compose,\n",
    "    Lambda,\n",
    "    RandomCrop,\n",
    "    RandomHorizontalFlip,\n",
    "    Resize,\n",
    ")\n",
    "\n",
    "\n",
    "class ApplyTransformToListUnderKey:\n",
    "    \"\"\"\n",
    "    Applies transform to key of dictionary input, where there is a list of values under it.\n",
    "\n",
    "    Args:\n",
    "        key (str): the dictionary key the transform is applied to\n",
    "        transform (callable): the transform that is applied for each element\n",
    "\n",
    "    Example:\n",
    "            transforms.ApplyTransformToKey(\n",
    "                key='video',\n",
    "                transform=UniformTemporalSubsample(num_video_samples),\n",
    "            )\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, key: str, transform: Callable):\n",
    "        self._key = key\n",
    "        self._transform = transform\n",
    "\n",
    "    def __call__(self, x: Dict[str, List[torch.Tensor]]) -> Dict[str, List[torch.Tensor]]:\n",
    "        for i in range(len(x[self._key])):\n",
    "            x[self._key][i] = self._transform(x[self._key][i])\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "mean = image_processor.image_mean\n",
    "std = image_processor.image_std\n",
    "if \"shortest_edge\" in image_processor.size:\n",
    "    height = width = image_processor.size[\"shortest_edge\"]\n",
    "else:\n",
    "    height = image_processor.size[\"height\"]\n",
    "    width = image_processor.size[\"width\"]\n",
    "resize_to = (height, width)\n",
    "\n",
    "num_frames_to_sample = trained_model.config.num_frames\n",
    "sample_rate = 4\n",
    "fps = 30\n",
    "clip_duration = num_frames_to_sample * sample_rate / fps\n",
    "\n",
    "# Validation and Test datasets' transformations.\n",
    "inference_transform = Compose(\n",
    "    [\n",
    "        ApplyTransformToListUnderKey(\n",
    "            key=\"video\",\n",
    "            transform=Compose(\n",
    "                [\n",
    "                    UniformTemporalSubsample(num_frames_to_sample),\n",
    "                    Lambda(lambda x: x / 255.0),\n",
    "                    Normalize(mean, std),\n",
    "                    Resize(resize_to),\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "MAX_VIDEO_DURATION = 6\n",
    "\n",
    "\n",
    "def build_evaluate_dataset(dataset_type: str):\n",
    "    # Validation and evaluation datasets.\n",
    "    dataset = pytorchvideo.data.Ucf101(\n",
    "        data_path=os.path.join(dataset_root_path, dataset_type),\n",
    "        clip_sampler=pytorchvideo.data.make_clip_sampler(\"random_multi\", clip_duration, 5),\n",
    "        decode_audio=False,\n",
    "        transform=inference_transform,\n",
    "    )\n",
    "    return dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def collate_fn(examples):\n",
    "    \"\"\"The collation function to be used by `Trainer` to prepare data batches.\"\"\"\n",
    "    # permute to (num_frames, num_channels, height, width)\n",
    "    pixel_values = torch.cat(\n",
    "        [torch.stack([single_example.permute(1, 0, 2, 3) for single_example in example[\"video\"]]) for example in\n",
    "         examples]\n",
    "    )\n",
    "    labels = torch.tensor([example[\"label\"] for example in examples])\n",
    "    # TODO - Maybe find a way to not unnecessarily duplicate those labels (They are duplicated just so the dimensions will fit with pixel_values, because torch tries to calculate loss for some reason  \n",
    "    labels = labels.repeat_interleave(len(examples[0][\"video\"]))\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"kuku\",\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    remove_unused_columns=False,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import statistics\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
    "import torch\n",
    "\n",
    "\n",
    "# source: https://jesusleal.io/2021/04/21/Longformer-multilabel-classification/\n",
    "def get_classification_report(predictions, labels):\n",
    "    f1_micro_average = f1_score(y_true=labels, y_pred=predictions, average='micro')\n",
    "    # roc_auc = roc_auc_score(y_true, y_pred, average='micro')\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    # return as dictionary\n",
    "    metrics = {'f1': f1_micro_average,\n",
    "               # 'roc_auc': roc_auc,\n",
    "               'accuracy': accuracy}\n",
    "    report = classification_report(labels, predictions, target_names=class_labels)\n",
    "    print(report)\n",
    "    return metrics\n",
    "\n",
    "\n",
    "# the compute_metrics function takes a Named Tuple as input:\n",
    "# predictions, which are the logits of the model as Numpy arrays,\n",
    "# and label_ids, which are the ground-truth labels as Numpy arrays.\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Computes accuracy on a batch of predictions.\"\"\"\n",
    "    n = eval_pred.label_ids.shape[0] // 5\n",
    "    predictions = [\n",
    "        statistics.mode(np.argmax(batch, axis=1))\n",
    "        for batch in np.array_split(eval_pred.predictions, n)\n",
    "    ]\n",
    "    labels = [batch[0] for batch in np.array_split(eval_pred.label_ids, n)]\n",
    "    return get_classification_report(predictions=predictions, labels=labels)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def build_trainer(dataset):\n",
    "    trainer = Trainer(\n",
    "        trained_model,\n",
    "        args,\n",
    "        eval_dataset=dataset,\n",
    "        tokenizer=image_processor,\n",
    "        compute_metrics=compute_metrics,\n",
    "        data_collator=collate_fn,\n",
    "    )\n",
    "    return trainer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for inference_dataset_type in ['val', 'test']:\n",
    "    # build dataset.\n",
    "    inference_dataset = build_evaluate_dataset(inference_dataset_type)\n",
    "    # build trainer.\n",
    "    inference_trainer = build_trainer(inference_dataset)\n",
    "    # print results\n",
    "    print(f\"---------{inference_dataset_type}---------\")\n",
    "    results = inference_trainer.evaluate(inference_dataset)\n",
    "    display(results)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
