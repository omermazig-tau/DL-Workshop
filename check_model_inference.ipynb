{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install pytorchvideo transformers evaluate accelerate decord -U"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "model_ckpt = \"omermazig/videomae-finetuned-nba-5-class-4-batch-8000-vid-multilabel-3\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "dataset_name = \"dataset\"\n",
    "\n",
    "try:\n",
    "    # For running on colab\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    root_path = pathlib.Path(f\"./drive/MyDrive/\")\n",
    "    dataset_root_path = root_path.joinpath(dataset_name)\n",
    "except ModuleNotFoundError:\n",
    "    # For running on PC\n",
    "    # dataset_root_path = pathlib.Path('UCF101_subset')\n",
    "    root_path = pathlib.Path(\".\")\n",
    "    dataset_root_path = pathlib.Path(r\"C:\\\\Users\\User\\Google Drive\").joinpath(dataset_name)\n",
    "    is_colab = False\n",
    "else:\n",
    "    is_colab = True"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pipe = pipeline(\"video-classification\", model=model_ckpt)\n",
    "trained_model = pipe.model\n",
    "image_processor = pipe.image_processor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from transformers import AutoImageProcessor, VideoMAEForVideoClassification\n",
    "# \n",
    "# \n",
    "# image_processor = AutoImageProcessor.from_pretrained(model_ckpt)\n",
    "# trained_model = VideoMAEForVideoClassification.from_pretrained(model_ckpt)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "video_extension = \"avi\"\n",
    "\n",
    "all_video_file_paths = (\n",
    "    list(dataset_root_path.glob(f\"train/*/*.{video_extension}\"))\n",
    "    + list(dataset_root_path.glob(f\"val/*/*.{video_extension}\"))\n",
    "    + list(dataset_root_path.glob(f\"test/*/*.{video_extension}\"))\n",
    ")\n",
    "all_video_file_paths[:5]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class_labels = sorted({path.parent.stem for path in all_video_file_paths})\n",
    "label2id = {label: i for i, label in enumerate(class_labels)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "id2multi_label = trained_model.config.id2label\n",
    "multi_label2id = trained_model.config.label2id\n",
    "class_mini_labels = list(id2multi_label.values())\n",
    "\n",
    "original_id2multi_id = {id_:[1 if mini_label in label else 0 for mini_label in class_mini_labels] for id_, label in id2label.items()}\n",
    "\n",
    "print(f\"Unique classes: {list(label2id.keys())}.\")\n",
    "print(f\"Unique mini classes: {list(multi_label2id.keys())}.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from typing import Callable, List, Dict\n",
    "import torch\n",
    "import pytorchvideo.data\n",
    "\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    Normalize,\n",
    "    RandomShortSideScale,\n",
    "    RemoveKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample,\n",
    ")\n",
    "\n",
    "from torchvision.transforms import (\n",
    "    Compose,\n",
    "    Lambda,\n",
    "    RandomCrop,\n",
    "    RandomHorizontalFlip,\n",
    "    Resize,\n",
    ")\n",
    "\n",
    "\n",
    "class ApplyTransformToListUnderKey:\n",
    "    \"\"\"\n",
    "    Applies transform to key of dictionary input, where there is a list of values under it.\n",
    "\n",
    "    Args:\n",
    "        key (str): the dictionary key the transform is applied to\n",
    "        transform (callable): the transform that is applied for each element\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, key: str, transform: Callable):\n",
    "        self._key = key\n",
    "        self._transform = transform\n",
    "\n",
    "    def __call__(self, x: Dict[str, List[torch.Tensor]]) -> Dict[str, List[torch.Tensor]]:\n",
    "        for i in range(len(x[self._key])):\n",
    "            x[self._key][i] = self._transform(x[self._key][i])\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pytorchvideo.data.labeled_video_paths import LabeledVideoPaths\n",
    "import torch\n",
    "from typing import Type, Optional, Callable, Dict, Any\n",
    "from pytorchvideo.data import ClipSampler, LabeledVideoDataset\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MultiLabelLabeledDataset(LabeledVideoDataset):\n",
    "    def __init__(self, data_path: str, clip_sampler: ClipSampler,\n",
    "                 video_sampler: Type[torch.utils.data.Sampler] = torch.utils.data.RandomSampler,\n",
    "                 transform: Optional[Callable[[Dict[str, Any]], Dict[str, Any]]] = None, video_path_prefix: str = \"\",\n",
    "                 decode_audio: bool = True, decoder: str = \"pyav\"):\n",
    "        labeled_video_paths = LabeledVideoPaths.from_path(data_path)\n",
    "        labeled_video_paths.path_prefix = video_path_prefix\n",
    "        super().__init__(labeled_video_paths, clip_sampler, video_sampler, transform, decode_audio, decoder)\n",
    "        self.original_id2multi_id = original_id2multi_id\n",
    "\n",
    "    def __next__(self):\n",
    "        example = super().__next__()\n",
    "        original_id = example[\"label\"]\n",
    "        multi_id = self.original_id2multi_id[original_id]\n",
    "        example[\"label\"] = multi_id\n",
    "        return example"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "mean = image_processor.image_mean\n",
    "std = image_processor.image_std\n",
    "if \"shortest_edge\" in image_processor.size:\n",
    "    height = width = image_processor.size[\"shortest_edge\"]\n",
    "else:\n",
    "    height = image_processor.size[\"height\"]\n",
    "    width = image_processor.size[\"width\"]\n",
    "resize_to = (height, width)\n",
    "\n",
    "num_frames_to_sample = trained_model.config.num_frames\n",
    "sample_rate = 4\n",
    "fps = 30\n",
    "clip_duration = num_frames_to_sample * sample_rate / fps\n",
    "\n",
    "# Validation and Test datasets' transformations.\n",
    "inference_transform = Compose(\n",
    "    [\n",
    "        ApplyTransformToListUnderKey(\n",
    "            key=\"video\",\n",
    "            transform=Compose(\n",
    "                [\n",
    "                    UniformTemporalSubsample(num_frames_to_sample),\n",
    "                    Lambda(lambda x: x / 255.0),\n",
    "                    Normalize(mean, std),\n",
    "                    RandomShortSideScale(min_size=256, max_size=320),\n",
    "                    RandomCrop(resize_to),\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "CLIPS_FROM_SINGLE_VIDEO = 5\n",
    "\n",
    "\n",
    "def build_evaluate_dataset(dataset_type: str):\n",
    "    # Validation and evaluation datasets.\n",
    "    dataset = MultiLabelLabeledDataset(\n",
    "        data_path=os.path.join(dataset_root_path, dataset_type),\n",
    "        clip_sampler=pytorchvideo.data.make_clip_sampler(\"random_multi\", clip_duration, CLIPS_FROM_SINGLE_VIDEO),\n",
    "        decode_audio=False,\n",
    "        transform=inference_transform,\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "val_dataset = build_evaluate_dataset(\"val\")\n",
    "test_dataset = build_evaluate_dataset(\"test\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def collate_fn(examples):\n",
    "    \"\"\"The collation function to be used by `Trainer` to prepare data batches.\"\"\"\n",
    "    # permute to (num_frames, num_channels, height, width)\n",
    "    if isinstance(examples[0][\"video\"], torch.Tensor):\n",
    "        # This is for training, where each training entry is a single video\n",
    "        pixel_values = torch.stack([example[\"video\"].permute(1, 0, 2, 3) for example in examples])\n",
    "    elif isinstance(examples[0][\"video\"], list):\n",
    "        # This is for evaluation, where each evaluation entry is multiple clips from a single video\n",
    "        pixel_values = torch.cat(\n",
    "            [torch.stack([single_example.permute(1, 0, 2, 3) for single_example in example[\"video\"]]) for example in\n",
    "             examples]\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Unrecognized input structure!\")\n",
    "        \n",
    "    labels = np.array([example[\"label\"] for example in examples], dtype=np.int64)\n",
    "    # TODO - Maybe find a way to not unnecessarily duplicate those labels (They are duplicated just so the dimensions will fit with pixel_values, because torch tries to calculate loss for some reason\n",
    "    if isinstance(examples[0][\"video\"], list):\n",
    "        # This is for evaluation, where each evaluation entry is multiple clips from a single video\n",
    "        labels = np.repeat(labels, CLIPS_FROM_SINGLE_VIDEO, axis=0)\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": torch.tensor(labels, dtype=torch.float)}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "batch_size = 4 if is_colab else 1\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"kuku\",\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    remove_unused_columns=False,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import statistics\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score, classification_report\n",
    "from transformers import EvalPrediction\n",
    "import torch\n",
    "\n",
    "\n",
    "# source: https://jesusleal.io/2021/04/21/Longformer-multilabel-classification/\n",
    "def multi_label_metrics(predictions, labels, threshold=0.5):\n",
    "    n = labels.shape[0] // CLIPS_FROM_SINGLE_VIDEO\n",
    "    predictions = [\n",
    "        np.average(batch, axis=0) for batch in np.array_split(predictions, n)\n",
    "    ]\n",
    "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.from_numpy(np.array(predictions)))\n",
    "    # next, use threshold to turn them into integer predictions\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= threshold)] = 1\n",
    "    # finally, compute metrics\n",
    "    y_true = [batch[0] for batch in np.array_split(labels, n)]\n",
    "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average='micro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    # return as dictionary\n",
    "    metrics = {'f1': f1_micro_average,\n",
    "               'roc_auc': roc_auc,\n",
    "               'accuracy': accuracy}\n",
    "    report = classification_report(y_true, y_pred, target_names=class_mini_labels)\n",
    "    print(report)\n",
    "    return metrics\n",
    "\n",
    "# the compute_metrics function takes a Named Tuple as input:\n",
    "# predictions, which are the logits of the model as Numpy arrays,\n",
    "# and label_ids, which are the ground-truth labels as Numpy arrays.\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Computes accuracy on a batch of predictions.\"\"\"\n",
    "    return multi_label_metrics(predictions=eval_pred.predictions, labels=eval_pred.label_ids)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def build_trainer(dataset):\n",
    "    trainer = Trainer(\n",
    "        trained_model,\n",
    "        args,\n",
    "        eval_dataset=dataset,\n",
    "        tokenizer=image_processor,\n",
    "        compute_metrics=compute_metrics,\n",
    "        data_collator=collate_fn,\n",
    "    )\n",
    "    return trainer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for inference_dataset_type in ['val', 'test']:\n",
    "    # build dataset.\n",
    "    inference_dataset = build_evaluate_dataset(inference_dataset_type)\n",
    "    # build trainer.\n",
    "    inference_trainer = build_trainer(inference_dataset)\n",
    "    # print results\n",
    "    print(f\"---------{inference_dataset_type}---------\")\n",
    "    results = inference_trainer.evaluate(inference_dataset)\n",
    "    display(results)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "try:\n",
    "    # For running on colab\n",
    "    from google.colab import runtime\n",
    "    # Wait for printing to sync with browser\n",
    "    import time;time.sleep(10)\n",
    "    runtime.unassign()\n",
    "except ModuleNotFoundError:\n",
    "    # I guess we're not on colab...\n",
    "    pass"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
