{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-08-09T16:38:42.405949300Z",
     "start_time": "2023-08-09T16:38:42.382951400Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pytorch_lightning\n",
    "import pytorchvideo.data\n",
    "import torch.utils.data\n",
    "\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    Normalize,\n",
    "    RandomShortSideScale,\n",
    "    RemoveKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample\n",
    ")\n",
    "\n",
    "from torchvision.transforms import (\n",
    "    Compose,\n",
    "    Lambda,\n",
    "    RandomCrop,\n",
    "    RandomHorizontalFlip\n",
    ")\n",
    "\n",
    "\n",
    "class NbaShotDataModule(pytorch_lightning.LightningDataModule):\n",
    "    # Dataset configuration\n",
    "    _DATA_PATH = 'new_dataset'\n",
    "    _CLIP_DURATION = 6  # Duration of sampled clip for each video\n",
    "    _BATCH_SIZE = 8\n",
    "    _NUM_WORKERS = 8  # Number of parallel processes fetching data\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        \"\"\"\n",
    "        Create the train partition. Add transform that subsamples and\n",
    "        normalizes the video before applying the scale, crop and flip augmentations.\n",
    "        \"\"\"\n",
    "        train_transform = Compose(\n",
    "            [\n",
    "                ApplyTransformToKey(\n",
    "                    key=\"video\",\n",
    "                    transform=Compose(\n",
    "                        [\n",
    "                            UniformTemporalSubsample(8),\n",
    "                            Lambda(lambda x: x / 255.0),\n",
    "                            Normalize((0.45, 0.45, 0.45), (0.225, 0.225, 0.225)),\n",
    "                            RandomShortSideScale(min_size=256, max_size=320),\n",
    "                            RandomCrop(244),\n",
    "                            RandomHorizontalFlip(p=0.5),\n",
    "                        ]\n",
    "                    ),\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        train_dataset = pytorchvideo.data.labeled_video_dataset(\n",
    "            data_path=os.path.join(self._DATA_PATH, \"train\"),\n",
    "            clip_sampler=pytorchvideo.data.make_clip_sampler(\"random\", self._CLIP_DURATION),\n",
    "            decode_audio=False,\n",
    "            transform=train_transform\n",
    "        )\n",
    "        return torch.utils.data.DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self._BATCH_SIZE,\n",
    "            num_workers=self._NUM_WORKERS,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        \"\"\"\n",
    "        Create the val partition. Add transform that subsamples and\n",
    "        normalizes the video before applying the scale, crop and flip augmentations.\n",
    "        \"\"\"\n",
    "        train_transform = Compose(\n",
    "            [\n",
    "                ApplyTransformToKey(\n",
    "                    key=\"video\",\n",
    "                    transform=Compose(\n",
    "                        [\n",
    "                            UniformTemporalSubsample(8),\n",
    "                            Lambda(lambda x: x / 255.0),\n",
    "                            Normalize((0.45, 0.45, 0.45), (0.225, 0.225, 0.225)),\n",
    "                        ]\n",
    "                    ),\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        train_dataset = pytorchvideo.data.labeled_video_dataset(\n",
    "            data_path=os.path.join(self._DATA_PATH, \"val\"),\n",
    "            clip_sampler=pytorchvideo.data.make_clip_sampler(\"uniform\", self._CLIP_DURATION),\n",
    "            decode_audio=False,\n",
    "            transform=train_transform\n",
    "        )\n",
    "        return torch.utils.data.DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self._BATCH_SIZE,\n",
    "            num_workers=self._NUM_WORKERS,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import pytorchvideo.models.resnet\n",
    "\n",
    "\n",
    "def make_resnet():\n",
    "    return pytorchvideo.models.resnet.create_resnet(\n",
    "        input_channel=3,  # RGB input from videos\n",
    "        model_depth=50,  # For the tutorial let's just use a 50 layer network\n",
    "        model_num_class=10,  # We have 400 classes\n",
    "        norm=nn.BatchNorm3d,\n",
    "        activation=nn.ReLU,\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-09T16:38:42.446077400Z",
     "start_time": "2023-08-09T16:38:42.398628400Z"
    }
   },
   "id": "11906cbd653ce3a5"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class VideoClassificationLightningModule(pytorch_lightning.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = make_resnet()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # The model expects a video tensor of shape (B, C, T, H, W), which is the\n",
    "        # format provided by the dataset\n",
    "        y_hat = self.model(batch[\"video\"])\n",
    "\n",
    "        # Compute cross entropy loss, loss.backwards will be called behind the scenes\n",
    "        # by PyTorchLightning after being returned from this method.\n",
    "        loss = F.cross_entropy(y_hat, batch[\"label\"])\n",
    "\n",
    "        # Log the train loss to Tensorboard\n",
    "        self.log(\"train_loss\", loss.item())\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        y_hat = self.model(batch[\"video\"])\n",
    "        loss = F.cross_entropy(y_hat, batch[\"label\"])\n",
    "        self.log(\"val_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"\n",
    "        Setup the Adam optimizer. Note, that this function also can return a lr scheduler, which is\n",
    "        usually useful for training video models.\n",
    "        \"\"\"\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-09T16:38:42.446077400Z",
     "start_time": "2023-08-09T16:38:42.414464400Z"
    }
   },
   "id": "250678ecfc59aab9"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def train():\n",
    "    classification_module = VideoClassificationLightningModule()\n",
    "    data_module = NbaShotDataModule()\n",
    "    trainer = pytorch_lightning.Trainer()\n",
    "    trainer.fit(classification_module, data_module)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-09T16:38:42.446077400Z",
     "start_time": "2023-08-09T16:38:42.436048700Z"
    }
   },
   "id": "6076783bca88c04f"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\loops\\utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type | Params\n",
      "-------------------------------\n",
      "0 | model | Net  | 31.7 M\n",
      "-------------------------------\n",
      "31.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "31.7 M    Total params\n",
      "126.695   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Sanity Checking: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cb0a1a477e284f3a865538adea1a1f5d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "Can't pickle local object 'NbaShotDataModule.val_dataloader.<locals>.<lambda>'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[12], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mdill\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpickle\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[9], line 5\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m()\u001B[0m\n\u001B[0;32m      3\u001B[0m data_module \u001B[38;5;241m=\u001B[39m NbaShotDataModule()\n\u001B[0;32m      4\u001B[0m trainer \u001B[38;5;241m=\u001B[39m pytorch_lightning\u001B[38;5;241m.\u001B[39mTrainer()\n\u001B[1;32m----> 5\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mclassification_module\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata_module\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:529\u001B[0m, in \u001B[0;36mTrainer.fit\u001B[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[0;32m    527\u001B[0m model \u001B[38;5;241m=\u001B[39m _maybe_unwrap_optimized(model)\n\u001B[0;32m    528\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39m_lightning_module \u001B[38;5;241m=\u001B[39m model\n\u001B[1;32m--> 529\u001B[0m \u001B[43mcall\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_and_handle_interrupt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    530\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit_impl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdatamodule\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\n\u001B[0;32m    531\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:42\u001B[0m, in \u001B[0;36m_call_and_handle_interrupt\u001B[1;34m(trainer, trainer_fn, *args, **kwargs)\u001B[0m\n\u001B[0;32m     40\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mlauncher \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m     41\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mlauncher\u001B[38;5;241m.\u001B[39mlaunch(trainer_fn, \u001B[38;5;241m*\u001B[39margs, trainer\u001B[38;5;241m=\u001B[39mtrainer, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m---> 42\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtrainer_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     44\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m _TunerExitException:\n\u001B[0;32m     45\u001B[0m     _call_teardown_hook(trainer)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:568\u001B[0m, in \u001B[0;36mTrainer._fit_impl\u001B[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[0;32m    558\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_data_connector\u001B[38;5;241m.\u001B[39mattach_data(\n\u001B[0;32m    559\u001B[0m     model, train_dataloaders\u001B[38;5;241m=\u001B[39mtrain_dataloaders, val_dataloaders\u001B[38;5;241m=\u001B[39mval_dataloaders, datamodule\u001B[38;5;241m=\u001B[39mdatamodule\n\u001B[0;32m    560\u001B[0m )\n\u001B[0;32m    562\u001B[0m ckpt_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_checkpoint_connector\u001B[38;5;241m.\u001B[39m_select_ckpt_path(\n\u001B[0;32m    563\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mfn,\n\u001B[0;32m    564\u001B[0m     ckpt_path,\n\u001B[0;32m    565\u001B[0m     model_provided\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m    566\u001B[0m     model_connected\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    567\u001B[0m )\n\u001B[1;32m--> 568\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mckpt_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    570\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mstopped\n\u001B[0;32m    571\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:973\u001B[0m, in \u001B[0;36mTrainer._run\u001B[1;34m(self, model, ckpt_path)\u001B[0m\n\u001B[0;32m    968\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_signal_connector\u001B[38;5;241m.\u001B[39mregister_signal_handlers()\n\u001B[0;32m    970\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[0;32m    971\u001B[0m \u001B[38;5;66;03m# RUN THE TRAINER\u001B[39;00m\n\u001B[0;32m    972\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[1;32m--> 973\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_stage\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    975\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[0;32m    976\u001B[0m \u001B[38;5;66;03m# POST-Training CLEAN UP\u001B[39;00m\n\u001B[0;32m    977\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[0;32m    978\u001B[0m log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: trainer tearing down\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1014\u001B[0m, in \u001B[0;36mTrainer._run_stage\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1012\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining:\n\u001B[0;32m   1013\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m isolate_rng():\n\u001B[1;32m-> 1014\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_sanity_check\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1015\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mset_detect_anomaly(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_detect_anomaly):\n\u001B[0;32m   1016\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfit_loop\u001B[38;5;241m.\u001B[39mrun()\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1043\u001B[0m, in \u001B[0;36mTrainer._run_sanity_check\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1040\u001B[0m call\u001B[38;5;241m.\u001B[39m_call_callback_hooks(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mon_sanity_check_start\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   1042\u001B[0m \u001B[38;5;66;03m# run eval step\u001B[39;00m\n\u001B[1;32m-> 1043\u001B[0m \u001B[43mval_loop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1045\u001B[0m call\u001B[38;5;241m.\u001B[39m_call_callback_hooks(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mon_sanity_check_end\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   1047\u001B[0m \u001B[38;5;66;03m# reset logger connector\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\loops\\utilities.py:177\u001B[0m, in \u001B[0;36m_no_grad_context.<locals>._decorator\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    175\u001B[0m     context_manager \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mno_grad\n\u001B[0;32m    176\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m context_manager():\n\u001B[1;32m--> 177\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mloop_run\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\loops\\evaluation_loop.py:98\u001B[0m, in \u001B[0;36m_EvaluationLoop.run\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     96\u001B[0m \u001B[38;5;129m@_no_grad_context\u001B[39m\n\u001B[0;32m     97\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrun\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m List[_OUT_DICT]:\n\u001B[1;32m---> 98\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msetup_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     99\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mskip:\n\u001B[0;32m    100\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m []\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\loops\\evaluation_loop.py:168\u001B[0m, in \u001B[0;36m_EvaluationLoop.setup_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    166\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_max_batches \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m    167\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m dl \u001B[38;5;129;01min\u001B[39;00m combined_loader\u001B[38;5;241m.\u001B[39mflattened:\n\u001B[1;32m--> 168\u001B[0m     \u001B[43m_check_dataloader_iterable\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msource\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrainer_fn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    169\u001B[0m     dl \u001B[38;5;241m=\u001B[39m _process_dataloader(trainer, dl)\n\u001B[0;32m    170\u001B[0m     dataloaders\u001B[38;5;241m.\u001B[39mappend(dl)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:385\u001B[0m, in \u001B[0;36m_check_dataloader_iterable\u001B[1;34m(dataloader, source, trainer_fn)\u001B[0m\n\u001B[0;32m    379\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_check_dataloader_iterable\u001B[39m(\n\u001B[0;32m    380\u001B[0m     dataloader: \u001B[38;5;28mobject\u001B[39m,\n\u001B[0;32m    381\u001B[0m     source: _DataLoaderSource,\n\u001B[0;32m    382\u001B[0m     trainer_fn: TrainerFn,\n\u001B[0;32m    383\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    384\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 385\u001B[0m         \u001B[38;5;28;43miter\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdataloader\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# type: ignore[call-overload]\u001B[39;00m\n\u001B[0;32m    386\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[0;32m    387\u001B[0m         \u001B[38;5;66;03m# A prefix in the message to disambiguate between the train- and (optional) val dataloader that .fit() accepts\u001B[39;00m\n\u001B[0;32m    388\u001B[0m         prefix \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain_\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m trainer_fn \u001B[38;5;241m==\u001B[39m TrainerFn\u001B[38;5;241m.\u001B[39mFITTING \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:441\u001B[0m, in \u001B[0;36mDataLoader.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    439\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iterator\n\u001B[0;32m    440\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 441\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_iterator\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:388\u001B[0m, in \u001B[0;36mDataLoader._get_iterator\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    386\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    387\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcheck_worker_number_rationality()\n\u001B[1;32m--> 388\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_MultiProcessingDataLoaderIter\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1042\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter.__init__\u001B[1;34m(self, loader)\u001B[0m\n\u001B[0;32m   1035\u001B[0m w\u001B[38;5;241m.\u001B[39mdaemon \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m   1036\u001B[0m \u001B[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001B[39;00m\n\u001B[0;32m   1037\u001B[0m \u001B[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001B[39;00m\n\u001B[0;32m   1038\u001B[0m \u001B[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001B[39;00m\n\u001B[0;32m   1039\u001B[0m \u001B[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001B[39;00m\n\u001B[0;32m   1040\u001B[0m \u001B[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001B[39;00m\n\u001B[0;32m   1041\u001B[0m \u001B[38;5;66;03m#     AssertionError: can only join a started process.\u001B[39;00m\n\u001B[1;32m-> 1042\u001B[0m \u001B[43mw\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstart\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1043\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_index_queues\u001B[38;5;241m.\u001B[39mappend(index_queue)\n\u001B[0;32m   1044\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_workers\u001B[38;5;241m.\u001B[39mappend(w)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\multiprocessing\\process.py:121\u001B[0m, in \u001B[0;36mBaseProcess.start\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    118\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _current_process\u001B[38;5;241m.\u001B[39m_config\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdaemon\u001B[39m\u001B[38;5;124m'\u001B[39m), \\\n\u001B[0;32m    119\u001B[0m        \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdaemonic processes are not allowed to have children\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m    120\u001B[0m _cleanup()\n\u001B[1;32m--> 121\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_popen \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_Popen\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    122\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sentinel \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_popen\u001B[38;5;241m.\u001B[39msentinel\n\u001B[0;32m    123\u001B[0m \u001B[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001B[39;00m\n\u001B[0;32m    124\u001B[0m \u001B[38;5;66;03m# reference to the process object (see bpo-30775)\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\multiprocessing\\context.py:224\u001B[0m, in \u001B[0;36mProcess._Popen\u001B[1;34m(process_obj)\u001B[0m\n\u001B[0;32m    222\u001B[0m \u001B[38;5;129m@staticmethod\u001B[39m\n\u001B[0;32m    223\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_Popen\u001B[39m(process_obj):\n\u001B[1;32m--> 224\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_default_context\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_context\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mProcess\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_Popen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprocess_obj\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\multiprocessing\\context.py:336\u001B[0m, in \u001B[0;36mSpawnProcess._Popen\u001B[1;34m(process_obj)\u001B[0m\n\u001B[0;32m    333\u001B[0m \u001B[38;5;129m@staticmethod\u001B[39m\n\u001B[0;32m    334\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_Popen\u001B[39m(process_obj):\n\u001B[0;32m    335\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpopen_spawn_win32\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Popen\n\u001B[1;32m--> 336\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mPopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprocess_obj\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\multiprocessing\\popen_spawn_win32.py:94\u001B[0m, in \u001B[0;36mPopen.__init__\u001B[1;34m(self, process_obj)\u001B[0m\n\u001B[0;32m     92\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     93\u001B[0m     reduction\u001B[38;5;241m.\u001B[39mdump(prep_data, to_child)\n\u001B[1;32m---> 94\u001B[0m     \u001B[43mreduction\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdump\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprocess_obj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mto_child\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     95\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     96\u001B[0m     set_spawning_popen(\u001B[38;5;28;01mNone\u001B[39;00m)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\multiprocessing\\reduction.py:60\u001B[0m, in \u001B[0;36mdump\u001B[1;34m(obj, file, protocol)\u001B[0m\n\u001B[0;32m     58\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdump\u001B[39m(obj, file, protocol\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m     59\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001B[39;00m\n\u001B[1;32m---> 60\u001B[0m     \u001B[43mForkingPickler\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprotocol\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdump\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mAttributeError\u001B[0m: Can't pickle local object 'NbaShotDataModule.val_dataloader.<locals>.<lambda>'"
     ]
    }
   ],
   "source": [
    "train()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-09T16:44:31.018848300Z",
     "start_time": "2023-08-09T16:44:28.821881100Z"
    }
   },
   "id": "d322dac83a5b5a4d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
